"""
Dash AI Server - Gemini LLM êµ¬í˜„ì²´

Google Native SDK (google-genai)ë¥¼ ì‚¬ìš©í•œ Gemini ëª¨ë¸ ë˜í¼
"""

import logging
from google import genai
from google.genai import types
from pydantic import BaseModel
from typing import TypeVar

from .base import BaseLLM

logger = logging.getLogger(__name__)

T = TypeVar('T', bound=BaseModel)


class GeminiLLM(BaseLLM):
    """Google Gemini LLM êµ¬í˜„ì²´ (Native SDK ê¸°ë°˜)
    
    Googleì˜ ê³µì‹ google-genai SDKë¥¼ ì‚¬ìš©í•˜ì—¬
    BaseLLM ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
    
    Example:
        llm = GeminiLLM(api_key="...", model="gemini-2.5-flash")
        result = llm.generate(prompt, ResponseSchema)
    """
    
    def __init__(
        self, 
        api_key: str, 
        model: str,
        max_tokens: int,
        thinking_level: str | None = None
    ):
        """
        Args:
            api_key: Google API Key
            model: ëª¨ë¸ëª…
            max_tokens: ìµœëŒ€ ì¶œë ¥ í† í°
            thinking_level: ì‚¬ê³  ìˆ˜ì¤€ (Noneì´ë©´ ë™ì , Gemini 3 ì „ìš©)
        """
        self.model = model
        self.max_tokens = max_tokens
        self.thinking_level = thinking_level
        self.client = genai.Client(api_key=api_key)
    
    def _log_token_usage(self, response, method_name: str = "") -> None:
        """í† í° ì‚¬ìš©ëŸ‰ ë¡œê¹…"""
        if hasattr(response, 'usage_metadata') and response.usage_metadata:
            usage = response.usage_metadata
            input_tokens = getattr(usage, 'prompt_token_count', 0) or 0
            output_tokens = getattr(usage, 'candidates_token_count', 0) or 0
            total_tokens = getattr(usage, 'total_token_count', 0) or 0
            thinking_tokens = getattr(usage, 'thoughts_token_count', 0) or 0
            
            # thinking tokensì´ ìˆìœ¼ë©´ í•¨ê»˜ í‘œì‹œ
            if thinking_tokens > 0:
                logger.info(
                    f"ğŸ“Š [{method_name}] í† í° ì‚¬ìš©ëŸ‰ - "
                    f"ì…ë ¥: {input_tokens}, ì¶œë ¥: {output_tokens}, "
                    f"ì‚¬ê³ : {thinking_tokens}, ì´í•©: {total_tokens}"
                )
            else:
                logger.info(
                    f"ğŸ“Š [{method_name}] í† í° ì‚¬ìš©ëŸ‰ - "
                    f"ì…ë ¥: {input_tokens}, ì¶œë ¥: {output_tokens}, ì´í•©: {total_tokens}"
                )
    
    def _build_config(
        self, 
        response_schema: type[BaseModel] | None = None,
        system_instruction: str | None = None
    ) -> types.GenerateContentConfig:
        """ê³µí†µ ì„¤ì • ë¹Œë“œ"""
        config_params = {
            "max_output_tokens": self.max_tokens,
        }
        
        if system_instruction:
            config_params["system_instruction"] = system_instruction
        
        if response_schema:
            config_params["response_mime_type"] = "application/json"
            config_params["response_schema"] = response_schema
        
        # thinking_levelì´ ì„¤ì •ëœ ê²½ìš°ì—ë§Œ ì¶”ê°€ (Gemini 3 ì „ìš©)
        if self.thinking_level:
            config_params["thinking_config"] = types.ThinkingConfig(
                thinking_level=self.thinking_level
            )
        
        return types.GenerateContentConfig(**config_params)
    
    def generate(
        self, 
        prompt: str, 
        response_schema: type[T],
        system_instruction: str | None = None
    ) -> T:
        """êµ¬ì¡°í™”ëœ JSON ì‘ë‹µ ìƒì„± (Native Structured Output)"""
        logger.info(f"ğŸš€ [generate] ìš”ì²­ ì‹œì‘ - ìŠ¤í‚¤ë§ˆ: {response_schema.__name__}")
        
        config = self._build_config(response_schema, system_instruction)
        
        response = self.client.models.generate_content(
            model=self.model,
            contents=prompt,
            config=config
        )
        
        self._log_token_usage(response, "generate")
        
        # JSON íŒŒì‹± ë° Pydantic ëª¨ë¸ ë³€í™˜
        result = response_schema.model_validate_json(response.text)
        return result
    
    def chat(
        self, 
        messages: list[dict],
        response_schema: type[T] | None = None,
        system_instruction: str | None = None
    ) -> str | T:
        """ëŒ€í™”í˜• ì‘ë‹µ ìƒì„±"""
        schema_name = response_schema.__name__ if response_schema else "ì—†ìŒ"
        logger.info(f"ğŸš€ [chat] ìš”ì²­ ì‹œì‘ - ë©”ì‹œì§€ ìˆ˜: {len(messages)}, ìŠ¤í‚¤ë§ˆ: {schema_name}")
        
        # ë©”ì‹œì§€ë¥¼ Native SDK í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        contents = []
        for msg in messages:
            role = "model" if msg["role"] == "assistant" else "user"
            contents.append(
                types.Content(
                    role=role,
                    parts=[types.Part(text=msg["content"])]
                )
            )
        
        config = self._build_config(response_schema, system_instruction)
        
        response = self.client.models.generate_content(
            model=self.model,
            contents=contents,
            config=config
        )
        
        self._log_token_usage(response, "chat")
        
        if response_schema:
            return response_schema.model_validate_json(response.text)
        else:
            return response.text
    
    def generate_text(self, prompt: str, system_instruction: str | None = None) -> str:
        """ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„±"""
        logger.info(f"ğŸš€ [generate_text] ìš”ì²­ ì‹œì‘ - í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")
        
        config = self._build_config(system_instruction=system_instruction)
        
        response = self.client.models.generate_content(
            model=self.model,
            contents=prompt,
            config=config
        )
        
        self._log_token_usage(response, "generate_text")
        return response.text
